{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Build 2 regression models, which predict 1) the return on investment 2) the IMDB rating of a movie.\n",
    "Inspect the freature importance to find out, how to most efficiently manipulate these targets. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "%reset -f\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.stats import uniform\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_cleaned_directors_filtered =  pd.read_csv(\"./output/movies_cleaned_directors_filtered.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create return on investment feature\n",
    "movies_cleaned_directors_filtered['return']  = (movies_cleaned_directors_filtered['revenue']-movies_cleaned_directors_filtered['budget']) / movies_cleaned_directors_filtered['budget'] # define the return of investment as multiplicative factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers with very unusual budgets/revenues\n",
    "min_budget = 50000 # set a minimum budget for movies to be considered here\n",
    "movies_cleaned_directors_filtered = movies_cleaned_directors_filtered[movies_cleaned_directors_filtered['budget']>=min_budget]\n",
    "# limit the max return\n",
    "max_return = 450 \n",
    "movies_cleaned_directors_filtered = movies_cleaned_directors_filtered[movies_cleaned_directors_filtered['return']<=max_return]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_cleaned_directors_filtered.sort_values('return',ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = movies_cleaned_directors_filtered[['runtimeMinutes','genres','averageRating','numVotes','directors','writers','original_language','budget','return']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with insufficient data\n",
    "df.dropna(axis=0, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column for each feature and convert \"genres\" column into 1 or 0 one-hot-encoding\n",
    "# get unique genres first\n",
    "dm = df['genres'].str.split(',', expand=True)\n",
    "# find unique genres\n",
    "g1 = dm[0].unique()\n",
    "g2 = dm[1].unique()\n",
    "g3 = dm[2].unique()\n",
    "g  = np.concatenate([g1,g2,g3])\n",
    "genre_list = pd.Series(g).unique()\n",
    "# remove nan values from list\n",
    "result = []\n",
    "for el in genre_list:\n",
    "    if type(el) == str:\n",
    "        result.append(el)\n",
    "genre_list = result\n",
    "# remove uninteresting genres - very few movies here\n",
    "genre_list.remove('Film-Noir')\n",
    "genre_list.remove('News')\n",
    "genre_list.remove('Sport')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new genres columns\n",
    "for genre in genre_list:\n",
    "    df[genre] = 0\n",
    "for row in range(len(df)):\n",
    "    for genre in genre_list:\n",
    "        if genre in df.loc[row, 'genres']:\n",
    "            df.loc[row, genre] = 1\n",
    "df.drop('genres',axis=1,inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the following features:\n",
    "\n",
    "# number of directors per film\n",
    "df['directors'] = df['directors'].apply(lambda row: len(row.split(',')))\n",
    "# number of writers per film\n",
    "df['writers'] = df['writers'].apply(lambda row: len(row.split(',')))\n",
    "# if a film is in original english or not\n",
    "df['foreign_language'] = df['original_language'].apply(lambda row: 0 if row=='en' else 1)\n",
    "df.drop('original_language',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data is highly scewed, we need to remove outliers\n",
    "sns.displot(data=df['return'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unusually high returns\n",
    "cols = ['return']\n",
    "\n",
    "Q1 = df[cols].quantile(0.00)\n",
    "Q3 = df[cols].quantile(0.97)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "df = df[~((df[cols] < (Q1 - 1.5 * IQR)) |(df[cols] > (Q3 + 1.5 * IQR))).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=df['return'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df['return'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=df['averageRating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['averageRating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Building"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Predicting Return on Investment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['return']\n",
    "X = df.drop('return', axis=1)\n",
    "# split into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_label = []\n",
    "estimator = []\n",
    "best_cv_score = []\n",
    "best_metric_score = []\n",
    "example_prob_result = []\n",
    "nonsense_prob_result = []\n",
    "# define parameters for ALL grid searches\n",
    "n_iter = 75\n",
    "scoring = 'neg_mean_squared_error' \n",
    "cv = 5  \n",
    "verbose = 1\n",
    "return_train_score = True\n",
    "random_state = 0\n",
    "n_jobs = -1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_label.append('linear')\n",
    "regressor = LinearRegression()\n",
    "pipe = Pipeline(steps=[(\"linear\", regressor)])\n",
    "\n",
    "# set up param distributions for grid search\n",
    "param_dist = {\n",
    "    \"linear__fit_intercept\": [False, True],\n",
    "}\n",
    "search = RandomizedSearchCV(pipe, param_dist, n_iter= n_iter, scoring=scoring,\n",
    "                            cv=cv, verbose=verbose, return_train_score=return_train_score,\n",
    "                            random_state=random_state)\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameter for grid search (CV score=%0.4f):\" % search.best_score_)\n",
    "print(search.best_params_)\n",
    "print(\"Training Set performance: \")\n",
    "y_pred = search.best_estimator_.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_pred))\n",
    "# save data for model comparison later\n",
    "estimator.append(search.best_estimator_)\n",
    "best_cv_score.append(search.best_score_)\n",
    "best_metric_score.append(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_label.append('lasso')\n",
    "regressor = Lasso(max_iter=2000)\n",
    "pipe = Pipeline(steps=[(\"lasso\", regressor)])\n",
    "\n",
    "# set up param distributions for grid search\n",
    "param_dist = {\n",
    "    \"lasso__fit_intercept\": [False, True],\n",
    "    \"lasso__alpha\": uniform(loc=0, scale=5).rvs(size=200),\n",
    "}\n",
    "search = RandomizedSearchCV(pipe, param_dist, n_iter= n_iter, scoring=scoring,\n",
    "                            cv=cv, verbose=verbose, return_train_score=return_train_score,\n",
    "                            random_state=random_state)\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameter for grid search (CV score=%0.4f):\" % search.best_score_)\n",
    "print(search.best_params_)\n",
    "print(\"Training Set performance: \")\n",
    "y_pred = search.best_estimator_.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_pred))\n",
    "# save data for model comparison later\n",
    "estimator.append(search.best_estimator_)\n",
    "best_cv_score.append(search.best_score_)\n",
    "best_metric_score.append(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_label.append('svr')\n",
    "# regressor = SVR(max_iter=-1)\n",
    "# pipe = Pipeline(steps=[(\"svr\", regressor)])\n",
    "\n",
    "# # set up param distributions for grid search\n",
    "# param_dist = {\n",
    "#     \"svr__C\": uniform(loc=0, scale=3).rvs(size=50),\n",
    "#     \"svr__kernel\": ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "#     \"svr__degree\": [2, 3, 4],\n",
    "#     \"svr__epsilon\": uniform(loc=0, scale=3).rvs(size=50),\n",
    "# }\n",
    "# search = RandomizedSearchCV(pipe, param_dist, n_iter=n_iter, scoring=scoring,\n",
    "#                             cv=cv, verbose=verbose, return_train_score=return_train_score,\n",
    "#                             random_state=random_state)\n",
    "\n",
    "# search.fit(X_train, y_train)\n",
    "\n",
    "# print(\"Best parameter for grid search (CV score=%0.4f):\" % search.best_score_)\n",
    "# print(search.best_params_)\n",
    "# print(\"Training Set performance: \")\n",
    "# y_pred = search.best_estimator_.predict(X_test)\n",
    "# print(mean_squared_error(y_test, y_pred))\n",
    "# #save data for model comparison later\n",
    "# estimator.append(search.best_estimator_)\n",
    "# best_cv_score.append(search.best_score_)\n",
    "# best_metric_score.append(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_label.append('decisiontree')\n",
    "regressor = DecisionTreeRegressor()\n",
    "pipe = Pipeline(steps=[(\"decisiontree\", regressor)])\n",
    "\n",
    "# set up param distributions for grid search\n",
    "depth = list(range(1,10,1))\n",
    "depth.append(None)\n",
    "param_dist = {\n",
    "    \"decisiontree__criterion\": ['absolute_error'],\n",
    "    \"decisiontree__max_depth\": depth,\n",
    "    \"decisiontree__min_samples_split\": list(range(2,200,1)),\n",
    "}\n",
    "search = RandomizedSearchCV(pipe, param_dist, n_iter= n_iter, scoring=scoring,\n",
    "                            cv=cv, verbose=verbose, return_train_score=return_train_score,\n",
    "                            random_state=random_state)\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameter for grid search (CV score=%0.4f):\" % search.best_score_)\n",
    "print(search.best_params_)\n",
    "print(\"Training Set performance: \")\n",
    "y_pred = search.best_estimator_.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_pred))\n",
    "#save data for model comparison later\n",
    "estimator.append(search.best_estimator_)\n",
    "best_cv_score.append(search.best_score_)\n",
    "best_metric_score.append(mean_squared_error(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_label.append('randomforest')\n",
    "regressor = RandomForestRegressor()\n",
    "pipe = Pipeline(steps=[(\"randomforest\", regressor)])\n",
    "\n",
    "# set up param distributions for grid search\n",
    "depth = list(range(1,10,1))\n",
    "depth.append(None)\n",
    "param_dist = {\n",
    "    \"randomforest__n_estimators\": list(range(1,1500,10)),\n",
    "    \"randomforest__max_depth\": depth,\n",
    "}\n",
    "search = RandomizedSearchCV(pipe, param_dist, n_iter= n_iter, scoring=scoring,\n",
    "                            cv=cv, verbose=verbose, return_train_score=return_train_score,\n",
    "                            random_state=random_state)\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameter for grid search (CV score=%0.4f):\" % search.best_score_)\n",
    "print(search.best_params_)\n",
    "print(\"Training Set performance: \")\n",
    "y_pred = search.best_estimator_.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_pred))\n",
    "#save data for model comparison later\n",
    "estimator.append(search.best_estimator_)\n",
    "best_cv_score.append(search.best_score_)\n",
    "best_metric_score.append(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# 21.8 28.26"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_label.append('xgboost')\n",
    "regressor = XGBRegressor(n_jobs=-1)\n",
    "pipe = Pipeline(steps=[(\"xgboost\", regressor)])\n",
    "\n",
    "# set up param distributions for grid search\n",
    "depth = list(range(1,10,1))\n",
    "depth.append(None)\n",
    "param_dist = {\n",
    "    \"xgboost__booster\": ['gbtree'],\n",
    "    \"xgboost__n_estimators\": list(range(1,300,1)),\n",
    "    \"xgboost__max_depth\": depth,\n",
    "    \"xgboost__min_child_weight\": list(range(0,150,1)),\n",
    "    \"xgboost__learning_rate\": uniform(loc=0, scale=0.2).rvs(size=25),\n",
    "}\n",
    "search = RandomizedSearchCV(pipe, param_dist, n_iter = 200, scoring=scoring,\n",
    "                            cv=cv, verbose=verbose, return_train_score=return_train_score,\n",
    "                            random_state=random_state)\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameter for grid search (CV score=%0.4f):\" % search.best_score_)\n",
    "print(search.best_params_)\n",
    "print(\"Training Set performance: \")\n",
    "y_pred = search.best_estimator_.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_pred))\n",
    "#save data for model comparison later\n",
    "estimator.append(search.best_estimator_)\n",
    "best_cv_score.append(search.best_score_)\n",
    "best_metric_score.append(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw histogram of importances of each feature\n",
    "model = search.best_estimator_[0]\n",
    "importances = pd.DataFrame(zip(model.feature_importances_,model.feature_names_in_), columns=['Importance', 'Feature'])\n",
    "importances.sort_values(by='Importance', inplace=True, ascending=False)\n",
    "plt.figure(figsize=(40, 20))\n",
    "sns.barplot(x='Feature', y='Importance', data=importances)\n",
    "\n",
    "importances.to_csv(\"./output/importances_return.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(14,14), sharey=True)\n",
    "colors = sns.color_palette(\"pastel\")\n",
    "\n",
    "axs[0].set_title(\"Model score comparisons\", fontsize=16)\n",
    "best_cv_score = [- k for k in best_cv_score]\n",
    "axs[0].bar(x=range(len(model_label)), height=(best_cv_score), width=0.5, color=colors)\n",
    "axs[1].bar(x=range(len(model_label)), height=best_metric_score, width=0.5, color=colors)\n",
    "\n",
    "for i in range(2):\n",
    "    axs[i].set_xticks(range(len(model_label)))\n",
    "    axs[i].set_xticklabels(labels=model_label, fontsize=13, rotation=0)\n",
    "\n",
    "axs[1].set_xlabel(\"Model\", fontsize=16)\n",
    "\n",
    "axs[0].set_ylabel(\"Best CV score\", fontsize=16)\n",
    "axs[1].set_ylabel(\"Accuracy vs. test data\", fontsize=16)\n",
    "\n",
    "#axs[0].set_ylim([0.45,1.05])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Predicting Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['averageRating']\n",
    "X = df.drop('averageRating', axis=1)\n",
    "# split into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_label = []\n",
    "estimator = []\n",
    "best_cv_score = []\n",
    "best_metric_score = []\n",
    "example_prob_result = []\n",
    "nonsense_prob_result = []\n",
    "# define parameters for ALL grid searches\n",
    "n_iter = 75\n",
    "scoring = 'neg_mean_squared_error' \n",
    "cv = 5  \n",
    "verbose = 1\n",
    "return_train_score = True\n",
    "random_state = 0\n",
    "n_jobs = -1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_label.append('linear')\n",
    "regressor = LinearRegression()\n",
    "pipe = Pipeline(steps=[(\"linear\", regressor)])\n",
    "\n",
    "# set up param distributions for grid search\n",
    "param_dist = {\n",
    "    \"linear__fit_intercept\": [False, True],\n",
    "}\n",
    "search = RandomizedSearchCV(pipe, param_dist, n_iter= n_iter, scoring=scoring,\n",
    "                            cv=cv, verbose=verbose, return_train_score=return_train_score,\n",
    "                            random_state=random_state)\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameter for grid search (CV score=%0.4f):\" % search.best_score_)\n",
    "print(search.best_params_)\n",
    "print(\"Training Set performance: \")\n",
    "y_pred = search.best_estimator_.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_pred))\n",
    "# save data for model comparison later\n",
    "estimator.append(search.best_estimator_)\n",
    "best_cv_score.append(search.best_score_)\n",
    "best_metric_score.append(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_label.append('lasso')\n",
    "regressor = Lasso(max_iter=2000)\n",
    "pipe = Pipeline(steps=[(\"lasso\", regressor)])\n",
    "\n",
    "# set up param distributions for grid search\n",
    "param_dist = {\n",
    "    \"lasso__fit_intercept\": [False, True],\n",
    "    \"lasso__alpha\": uniform(loc=0, scale=5).rvs(size=200),\n",
    "}\n",
    "search = RandomizedSearchCV(pipe, param_dist, n_iter= n_iter, scoring=scoring,\n",
    "                            cv=cv, verbose=verbose, return_train_score=return_train_score,\n",
    "                            random_state=random_state)\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameter for grid search (CV score=%0.4f):\" % search.best_score_)\n",
    "print(search.best_params_)\n",
    "print(\"Training Set performance: \")\n",
    "y_pred = search.best_estimator_.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_pred))\n",
    "# save data for model comparison later\n",
    "estimator.append(search.best_estimator_)\n",
    "best_cv_score.append(search.best_score_)\n",
    "best_metric_score.append(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_label.append('svr')\n",
    "# regressor = SVR(max_iter=-1)\n",
    "# pipe = Pipeline(steps=[(\"svr\", regressor)])\n",
    "\n",
    "# # set up param distributions for grid search\n",
    "# param_dist = {\n",
    "#     \"svr__C\": uniform(loc=0, scale=3).rvs(size=50),\n",
    "#     \"svr__kernel\": ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "#     \"svr__degree\": [2, 3, 4],\n",
    "#     \"svr__epsilon\": uniform(loc=0, scale=3).rvs(size=50),\n",
    "# }\n",
    "# search = RandomizedSearchCV(pipe, param_dist, n_iter=n_iter, scoring=scoring,\n",
    "#                             cv=cv, verbose=verbose, return_train_score=return_train_score,\n",
    "#                             random_state=random_state)\n",
    "\n",
    "# search.fit(X_train, y_train)\n",
    "\n",
    "# print(\"Best parameter for grid search (CV score=%0.4f):\" % search.best_score_)\n",
    "# print(search.best_params_)\n",
    "# print(\"Training Set performance: \")\n",
    "# y_pred = search.best_estimator_.predict(X_test)\n",
    "# print(mean_squared_error(y_test, y_pred))\n",
    "# #save data for model comparison later\n",
    "# estimator.append(search.best_estimator_)\n",
    "# best_cv_score.append(search.best_score_)\n",
    "# best_metric_score.append(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_label.append('decisiontree')\n",
    "regressor = DecisionTreeRegressor()\n",
    "pipe = Pipeline(steps=[(\"decisiontree\", regressor)])\n",
    "\n",
    "# set up param distributions for grid search\n",
    "depth = list(range(1,10,1))\n",
    "depth.append(None)\n",
    "param_dist = {\n",
    "    \"decisiontree__criterion\": ['absolute_error'],\n",
    "    \"decisiontree__max_depth\": depth,\n",
    "    \"decisiontree__min_samples_split\": list(range(2,200,1)),\n",
    "}\n",
    "search = RandomizedSearchCV(pipe, param_dist, n_iter= n_iter, scoring=scoring,\n",
    "                            cv=cv, verbose=verbose, return_train_score=return_train_score,\n",
    "                            random_state=random_state)\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameter for grid search (CV score=%0.4f):\" % search.best_score_)\n",
    "print(search.best_params_)\n",
    "print(\"Training Set performance: \")\n",
    "y_pred = search.best_estimator_.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_pred))\n",
    "#save data for model comparison later\n",
    "estimator.append(search.best_estimator_)\n",
    "best_cv_score.append(search.best_score_)\n",
    "best_metric_score.append(mean_squared_error(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_label.append('randomforest')\n",
    "regressor = RandomForestRegressor()\n",
    "pipe = Pipeline(steps=[(\"randomforest\", regressor)])\n",
    "\n",
    "# set up param distributions for grid search\n",
    "depth = list(range(1,10,1))\n",
    "depth.append(None)\n",
    "param_dist = {\n",
    "    \"randomforest__n_estimators\": list(range(1,1500,10)),\n",
    "    \"randomforest__max_depth\": depth,\n",
    "}\n",
    "search = RandomizedSearchCV(pipe, param_dist, n_iter= n_iter, scoring=scoring,\n",
    "                            cv=cv, verbose=verbose, return_train_score=return_train_score,\n",
    "                            random_state=random_state)\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameter for grid search (CV score=%0.4f):\" % search.best_score_)\n",
    "print(search.best_params_)\n",
    "print(\"Training Set performance: \")\n",
    "y_pred = search.best_estimator_.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_pred))\n",
    "#save data for model comparison later\n",
    "estimator.append(search.best_estimator_)\n",
    "best_cv_score.append(search.best_score_)\n",
    "best_metric_score.append(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# 21.8 28.26"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_label.append('xgboost')\n",
    "regressor = XGBRegressor(n_jobs=-1)\n",
    "pipe = Pipeline(steps=[(\"xgboost\", regressor)])\n",
    "\n",
    "# set up param distributions for grid search\n",
    "depth = list(range(1,10,1))\n",
    "depth.append(None)\n",
    "param_dist = {\n",
    "    \"xgboost__booster\": ['gbtree'],\n",
    "    \"xgboost__n_estimators\": list(range(1,300,1)),\n",
    "    \"xgboost__max_depth\": depth,\n",
    "    \"xgboost__min_child_weight\": list(range(0,150,1)),\n",
    "    \"xgboost__learning_rate\": uniform(loc=0, scale=0.2).rvs(size=25),\n",
    "}\n",
    "search = RandomizedSearchCV(pipe, param_dist, n_iter = 200, scoring=scoring,\n",
    "                            cv=cv, verbose=verbose, return_train_score=return_train_score,\n",
    "                            random_state=random_state)\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameter for grid search (CV score=%0.4f):\" % search.best_score_)\n",
    "print(search.best_params_)\n",
    "print(\"Training Set performance: \")\n",
    "y_pred = search.best_estimator_.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_pred))\n",
    "#save data for model comparison later\n",
    "estimator.append(search.best_estimator_)\n",
    "best_cv_score.append(search.best_score_)\n",
    "best_metric_score.append(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw histogram of importances of each feature\n",
    "model = search.best_estimator_[0]\n",
    "importances = pd.DataFrame(zip(model.feature_importances_,model.feature_names_in_), columns=['Importance', 'Feature'])\n",
    "importances.sort_values(by='Importance', inplace=True, ascending=False)\n",
    "plt.figure(figsize=(40, 20))\n",
    "sns.barplot(x='Feature', y='Importance', data=importances)\n",
    "\n",
    "importances.to_csv(\"./output/importances_rating.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(14,14), sharey=True)\n",
    "colors = sns.color_palette(\"pastel\")\n",
    "\n",
    "axs[0].set_title(\"Model score comparisons\", fontsize=16)\n",
    "best_cv_score = [- k for k in best_cv_score]\n",
    "axs[0].bar(x=range(len(model_label)), height=(best_cv_score), width=0.5, color=colors)\n",
    "axs[1].bar(x=range(len(model_label)), height=best_metric_score, width=0.5, color=colors)\n",
    "\n",
    "for i in range(2):\n",
    "    axs[i].set_xticks(range(len(model_label)))\n",
    "    axs[i].set_xticklabels(labels=model_label, fontsize=13, rotation=0)\n",
    "\n",
    "axs[1].set_xlabel(\"Model\", fontsize=16)\n",
    "\n",
    "axs[0].set_ylabel(\"Best CV score\", fontsize=16)\n",
    "axs[1].set_ylabel(\"Accuracy vs. test data\", fontsize=16)\n",
    "\n",
    "#axs[0].set_ylim([0.45,1.05])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6deebe2b6c7a3fc5cd075ffe89f3ba7e7310f85d9af63e76c7275543219241fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
